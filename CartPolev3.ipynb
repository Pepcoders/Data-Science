{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-rl2, tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import rl\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# balance exploration and exploitation\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "# to decay our eps\n",
    "from rl.policy import LinearAnnealedPolicy\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state_size = env.observation_space.shape[0]  # total number of states (S)\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(state_size, num_actions):\n",
    "    input = Input(shape=(1,state_size))\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    output = Dense(num_actions, activation='linear')(x)\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1, 4)]            0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=build_model(state_size, action_size)\n",
    "# model = tf.keras.models.load_model('./models/model.h5') to load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=action_size, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(optimizer= Adam(learning_rate=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2500 steps ...\n",
      "   48/2500: episode: 1, duration: 0.284s, episode steps:  48, steps per second: 169, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 6.639272, mae: 31.279091, mean_q: 64.606182, mean_eps: 0.997390\n",
      "   56/2500: episode: 2, duration: 0.049s, episode steps:   8, steps per second: 162, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 4.832373, mae: 31.562858, mean_q: 64.874635, mean_eps: 0.995365\n",
      "   74/2500: episode: 3, duration: 0.093s, episode steps:  18, steps per second: 194, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.981705, mae: 31.341058, mean_q: 65.177416, mean_eps: 0.994195\n",
      "  105/2500: episode: 4, duration: 0.165s, episode steps:  31, steps per second: 188, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 5.949092, mae: 32.209021, mean_q: 66.469017, mean_eps: 0.991990\n",
      "  130/2500: episode: 5, duration: 0.130s, episode steps:  25, steps per second: 193, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 6.353478, mae: 32.791020, mean_q: 67.337390, mean_eps: 0.989470\n",
      "  163/2500: episode: 6, duration: 0.177s, episode steps:  33, steps per second: 186, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 6.871281, mae: 31.812510, mean_q: 65.241153, mean_eps: 0.986860\n",
      "  193/2500: episode: 7, duration: 0.192s, episode steps:  30, steps per second: 157, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 5.771608, mae: 32.510266, mean_q: 66.853311, mean_eps: 0.984025\n",
      "  214/2500: episode: 8, duration: 0.116s, episode steps:  21, steps per second: 181, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 5.061501, mae: 32.845297, mean_q: 67.931893, mean_eps: 0.981730\n",
      "  226/2500: episode: 9, duration: 0.064s, episode steps:  12, steps per second: 188, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.895093, mae: 31.476342, mean_q: 65.188258, mean_eps: 0.980245\n",
      "  236/2500: episode: 10, duration: 0.051s, episode steps:  10, steps per second: 194, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.718608, mae: 33.378403, mean_q: 68.768745, mean_eps: 0.979255\n",
      "  249/2500: episode: 11, duration: 0.067s, episode steps:  13, steps per second: 195, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 4.049517, mae: 32.690430, mean_q: 67.640788, mean_eps: 0.978220\n",
      "  270/2500: episode: 12, duration: 0.117s, episode steps:  21, steps per second: 180, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 6.517836, mae: 32.889956, mean_q: 68.466694, mean_eps: 0.976690\n",
      "  296/2500: episode: 13, duration: 0.128s, episode steps:  26, steps per second: 203, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 9.399692, mae: 32.863342, mean_q: 67.594073, mean_eps: 0.974575\n",
      "  318/2500: episode: 14, duration: 0.116s, episode steps:  22, steps per second: 190, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.914117, mae: 33.096248, mean_q: 68.057322, mean_eps: 0.972415\n",
      "  332/2500: episode: 15, duration: 0.080s, episode steps:  14, steps per second: 175, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 4.884633, mae: 32.450315, mean_q: 67.209400, mean_eps: 0.970795\n",
      "  352/2500: episode: 16, duration: 0.107s, episode steps:  20, steps per second: 187, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 6.081024, mae: 33.234110, mean_q: 68.483413, mean_eps: 0.969265\n",
      "  380/2500: episode: 17, duration: 0.160s, episode steps:  28, steps per second: 175, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 5.492594, mae: 33.447220, mean_q: 69.162592, mean_eps: 0.967105\n",
      "  391/2500: episode: 18, duration: 0.058s, episode steps:  11, steps per second: 191, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 5.044779, mae: 32.511028, mean_q: 67.338051, mean_eps: 0.965350\n",
      "  408/2500: episode: 19, duration: 0.085s, episode steps:  17, steps per second: 199, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.717726, mae: 33.839654, mean_q: 69.846296, mean_eps: 0.964090\n",
      "  422/2500: episode: 20, duration: 0.070s, episode steps:  14, steps per second: 199, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 7.473755, mae: 32.873342, mean_q: 68.320153, mean_eps: 0.962695\n",
      "  438/2500: episode: 21, duration: 0.093s, episode steps:  16, steps per second: 171, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 6.565222, mae: 33.365039, mean_q: 69.223684, mean_eps: 0.961345\n",
      "  449/2500: episode: 22, duration: 0.057s, episode steps:  11, steps per second: 193, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 11.185353, mae: 31.753340, mean_q: 67.017221, mean_eps: 0.960130\n",
      "  464/2500: episode: 23, duration: 0.077s, episode steps:  15, steps per second: 194, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 4.923634, mae: 33.214008, mean_q: 68.438689, mean_eps: 0.958960\n",
      "  475/2500: episode: 24, duration: 0.065s, episode steps:  11, steps per second: 169, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.188495, mae: 33.635986, mean_q: 69.781444, mean_eps: 0.957790\n",
      "  547/2500: episode: 25, duration: 0.366s, episode steps:  72, steps per second: 196, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.490497, mae: 33.404428, mean_q: 69.323044, mean_eps: 0.954055\n",
      "  559/2500: episode: 26, duration: 0.067s, episode steps:  12, steps per second: 178, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 6.416564, mae: 34.639100, mean_q: 71.501958, mean_eps: 0.950275\n",
      "  580/2500: episode: 27, duration: 0.104s, episode steps:  21, steps per second: 202, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 7.436886, mae: 33.776996, mean_q: 69.890389, mean_eps: 0.948790\n",
      "  597/2500: episode: 28, duration: 0.097s, episode steps:  17, steps per second: 174, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 4.992673, mae: 34.072373, mean_q: 70.755975, mean_eps: 0.947080\n",
      "  635/2500: episode: 29, duration: 0.188s, episode steps:  38, steps per second: 202, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 7.273327, mae: 34.399652, mean_q: 71.242023, mean_eps: 0.944605\n",
      "  654/2500: episode: 30, duration: 0.111s, episode steps:  19, steps per second: 172, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.263 [0.000, 1.000],  loss: 7.472222, mae: 35.090771, mean_q: 71.783493, mean_eps: 0.942040\n",
      "  672/2500: episode: 31, duration: 0.088s, episode steps:  18, steps per second: 204, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 6.168323, mae: 34.115067, mean_q: 70.628012, mean_eps: 0.940375\n",
      "  688/2500: episode: 32, duration: 0.084s, episode steps:  16, steps per second: 191, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 9.119320, mae: 34.363403, mean_q: 71.092825, mean_eps: 0.938845\n",
      "  716/2500: episode: 33, duration: 0.146s, episode steps:  28, steps per second: 191, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 4.112094, mae: 33.821996, mean_q: 70.339043, mean_eps: 0.936865\n",
      "  742/2500: episode: 34, duration: 0.138s, episode steps:  26, steps per second: 188, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 9.021849, mae: 34.148889, mean_q: 71.137892, mean_eps: 0.934435\n",
      "  792/2500: episode: 35, duration: 0.251s, episode steps:  50, steps per second: 199, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 5.056829, mae: 34.966481, mean_q: 72.616297, mean_eps: 0.931015\n",
      "  830/2500: episode: 36, duration: 0.184s, episode steps:  38, steps per second: 206, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 9.482465, mae: 35.104742, mean_q: 72.363286, mean_eps: 0.927055\n",
      "  848/2500: episode: 37, duration: 0.095s, episode steps:  18, steps per second: 189, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 7.605051, mae: 35.422082, mean_q: 73.317367, mean_eps: 0.924535\n",
      "  870/2500: episode: 38, duration: 0.107s, episode steps:  22, steps per second: 206, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.017204, mae: 35.524454, mean_q: 73.023352, mean_eps: 0.922735\n",
      "  904/2500: episode: 39, duration: 0.172s, episode steps:  34, steps per second: 198, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.382 [0.000, 1.000],  loss: 9.196858, mae: 34.978124, mean_q: 72.775883, mean_eps: 0.920215\n",
      "  923/2500: episode: 40, duration: 0.099s, episode steps:  19, steps per second: 192, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 8.734814, mae: 35.084893, mean_q: 72.217338, mean_eps: 0.917830\n",
      "  935/2500: episode: 41, duration: 0.082s, episode steps:  12, steps per second: 146, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 3.086360, mae: 35.155971, mean_q: 72.839979, mean_eps: 0.916435\n",
      "  949/2500: episode: 42, duration: 0.088s, episode steps:  14, steps per second: 160, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 3.728594, mae: 35.637443, mean_q: 73.734634, mean_eps: 0.915265\n",
      "  978/2500: episode: 43, duration: 0.142s, episode steps:  29, steps per second: 204, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.328588, mae: 35.733820, mean_q: 73.760599, mean_eps: 0.913330\n",
      " 1006/2500: episode: 44, duration: 0.143s, episode steps:  28, steps per second: 196, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 5.224936, mae: 36.088156, mean_q: 74.346469, mean_eps: 0.910765\n",
      " 1052/2500: episode: 45, duration: 0.226s, episode steps:  46, steps per second: 203, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 6.343071, mae: 36.320463, mean_q: 75.131458, mean_eps: 0.907435\n",
      " 1089/2500: episode: 46, duration: 0.181s, episode steps:  37, steps per second: 204, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 8.812511, mae: 36.184085, mean_q: 74.535419, mean_eps: 0.903700\n",
      " 1145/2500: episode: 47, duration: 0.292s, episode steps:  56, steps per second: 192, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  loss: 6.327498, mae: 36.366567, mean_q: 75.178407, mean_eps: 0.899515\n",
      " 1169/2500: episode: 48, duration: 0.117s, episode steps:  24, steps per second: 205, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 11.948325, mae: 35.342627, mean_q: 73.159885, mean_eps: 0.895915\n",
      " 1184/2500: episode: 49, duration: 0.075s, episode steps:  15, steps per second: 201, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 4.677500, mae: 36.792915, mean_q: 76.201887, mean_eps: 0.894160\n",
      " 1248/2500: episode: 50, duration: 0.309s, episode steps:  64, steps per second: 207, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 5.082473, mae: 37.082698, mean_q: 76.858321, mean_eps: 0.890605\n",
      " 1269/2500: episode: 51, duration: 0.108s, episode steps:  21, steps per second: 195, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 8.668063, mae: 36.970467, mean_q: 76.437824, mean_eps: 0.886780\n",
      " 1286/2500: episode: 52, duration: 0.094s, episode steps:  17, steps per second: 181, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 9.615435, mae: 36.944509, mean_q: 77.033861, mean_eps: 0.885070\n",
      " 1320/2500: episode: 53, duration: 0.188s, episode steps:  34, steps per second: 181, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 2.440917, mae: 37.116843, mean_q: 77.363916, mean_eps: 0.882775\n",
      " 1355/2500: episode: 54, duration: 0.194s, episode steps:  35, steps per second: 181, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.629 [0.000, 1.000],  loss: 8.319769, mae: 37.621137, mean_q: 78.098095, mean_eps: 0.879670\n",
      " 1400/2500: episode: 55, duration: 0.216s, episode steps:  45, steps per second: 208, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 7.768076, mae: 37.138446, mean_q: 76.569236, mean_eps: 0.876070\n",
      " 1425/2500: episode: 56, duration: 0.125s, episode steps:  25, steps per second: 199, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 7.753414, mae: 37.341694, mean_q: 77.653002, mean_eps: 0.872920\n",
      " 1440/2500: episode: 57, duration: 0.074s, episode steps:  15, steps per second: 203, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.273946, mae: 37.513128, mean_q: 77.880170, mean_eps: 0.871120\n",
      " 1474/2500: episode: 58, duration: 0.171s, episode steps:  34, steps per second: 199, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.857824, mae: 38.643948, mean_q: 79.951812, mean_eps: 0.868915\n",
      " 1525/2500: episode: 59, duration: 0.281s, episode steps:  51, steps per second: 181, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 6.157900, mae: 37.974040, mean_q: 78.701543, mean_eps: 0.865090\n",
      " 1548/2500: episode: 60, duration: 0.133s, episode steps:  23, steps per second: 173, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 3.630827, mae: 38.871623, mean_q: 80.571657, mean_eps: 0.861760\n",
      " 1567/2500: episode: 61, duration: 0.109s, episode steps:  19, steps per second: 174, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 4.263224, mae: 38.737934, mean_q: 80.768156, mean_eps: 0.859870\n",
      " 1586/2500: episode: 62, duration: 0.094s, episode steps:  19, steps per second: 203, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.137411, mae: 37.905244, mean_q: 79.676060, mean_eps: 0.858160\n",
      " 1626/2500: episode: 63, duration: 0.203s, episode steps:  40, steps per second: 197, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.575 [0.000, 1.000],  loss: 4.186626, mae: 39.763558, mean_q: 82.026705, mean_eps: 0.855505\n",
      " 1654/2500: episode: 64, duration: 0.141s, episode steps:  28, steps per second: 199, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 10.380611, mae: 39.359245, mean_q: 80.877258, mean_eps: 0.852445\n",
      " 1664/2500: episode: 65, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 8.601368, mae: 38.917977, mean_q: 79.729910, mean_eps: 0.850735\n",
      " 1703/2500: episode: 66, duration: 0.433s, episode steps:  39, steps per second:  90, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 9.710050, mae: 39.009695, mean_q: 80.892969, mean_eps: 0.848530\n",
      " 1732/2500: episode: 67, duration: 0.304s, episode steps:  29, steps per second:  95, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 14.072296, mae: 39.827173, mean_q: 81.767236, mean_eps: 0.845470\n",
      " 1779/2500: episode: 68, duration: 0.500s, episode steps:  47, steps per second:  94, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.383 [0.000, 1.000],  loss: 7.991690, mae: 39.244144, mean_q: 81.040922, mean_eps: 0.842050\n",
      " 1813/2500: episode: 69, duration: 0.365s, episode steps:  34, steps per second:  93, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.140580, mae: 39.330065, mean_q: 81.174884, mean_eps: 0.838405\n",
      " 1848/2500: episode: 70, duration: 0.379s, episode steps:  35, steps per second:  92, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 9.875845, mae: 40.092548, mean_q: 82.692072, mean_eps: 0.835300\n",
      " 1880/2500: episode: 71, duration: 0.382s, episode steps:  32, steps per second:  84, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  loss: 4.584363, mae: 40.293386, mean_q: 83.457608, mean_eps: 0.832285\n",
      " 1897/2500: episode: 72, duration: 0.207s, episode steps:  17, steps per second:  82, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 16.951477, mae: 39.601199, mean_q: 81.237698, mean_eps: 0.830080\n",
      " 1950/2500: episode: 73, duration: 0.619s, episode steps:  53, steps per second:  86, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 7.809993, mae: 40.087440, mean_q: 82.522186, mean_eps: 0.826930\n",
      " 1981/2500: episode: 74, duration: 0.332s, episode steps:  31, steps per second:  93, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 17.926127, mae: 40.177677, mean_q: 82.293216, mean_eps: 0.823150\n",
      " 2031/2500: episode: 75, duration: 0.523s, episode steps:  50, steps per second:  96, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 12.755865, mae: 40.319419, mean_q: 82.966052, mean_eps: 0.819505\n",
      " 2047/2500: episode: 76, duration: 0.184s, episode steps:  16, steps per second:  87, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 10.495555, mae: 40.009850, mean_q: 82.641022, mean_eps: 0.816535\n",
      " 2067/2500: episode: 77, duration: 0.218s, episode steps:  20, steps per second:  92, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3.397550, mae: 39.570583, mean_q: 82.229358, mean_eps: 0.814915\n",
      " 2087/2500: episode: 78, duration: 0.209s, episode steps:  20, steps per second:  96, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 3.001992, mae: 41.094200, mean_q: 85.169247, mean_eps: 0.813115\n",
      " 2165/2500: episode: 79, duration: 0.886s, episode steps:  78, steps per second:  88, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.403348, mae: 40.803719, mean_q: 84.463663, mean_eps: 0.808705\n",
      " 2178/2500: episode: 80, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 10.835308, mae: 40.104686, mean_q: 82.598524, mean_eps: 0.804610\n",
      " 2190/2500: episode: 81, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 24.068649, mae: 41.505929, mean_q: 86.302111, mean_eps: 0.803485\n",
      " 2339/2500: episode: 82, duration: 1.559s, episode steps: 149, steps per second:  96, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 11.541419, mae: 41.425152, mean_q: 85.820625, mean_eps: 0.796240\n",
      " 2357/2500: episode: 83, duration: 0.103s, episode steps:  18, steps per second: 174, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 12.555730, mae: 41.719950, mean_q: 86.018046, mean_eps: 0.788725\n",
      "done, took 17.064 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f54146370>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no need to fit if using saved model\n",
    "dqn.fit(env, nb_steps=2500,\n",
    "visualize=False,\n",
    "verbose=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 180.000, steps: 180\n",
      "Episode 10: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f541465b0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f54d5487fc61c32ae7f66b51dc98d911dfe184923824cf696ce9cc457d5a9759"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
